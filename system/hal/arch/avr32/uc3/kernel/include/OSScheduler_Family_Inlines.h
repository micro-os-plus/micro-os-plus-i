/*
 *      Copyright (C) 2011 Liviu Ionescu.
 *
 *	This file is part of the uOS++ distribution.
 *
 *	Many thanks to the FreeRTOS developers for providing great
 *	inspiration and functional pieces of code.
 */

#ifndef HAL_FAMILY_OSSCHEDULER_INLINES_H_
#define HAL_FAMILY_OSSCHEDULER_INLINES_H_

//----------------------------------------------------------------------------

/*
 * Overview
 *
 * These definitions handle the operations required to save and restore the
 * current task 'context' to/from the task control block (TCB).
 *
 * The processor registers are pushed onto/popped from the current task stack,
 * and only the resulting stack pointer (SP) is stored in the current task area.
 *
 * To make things simpler and faster, the code uses the global variable
 * g_ppCurrentStack that points to the current OSTask::m_pStack, where
 * the pointer to the current stack frame is located. This variable is
 * set during the context switch in OSScheduler::contextSwitch().
 *
 * - contextSave() : store the SP through the global pointer
 *      *g_ppCurrentStack = SP
 *
 * - contextRestore() : load the SP through the global pointer
 *      SP = *g_ppCurrentStack
 */

/*
 * AVR32 Specifics
 *
 * Since on AVR32 there is no special, unified, support for context switching,
 * (like on ARM Cortex M3/M4), we have two distinct context switching
 * situations:
 *
 * - the yield() code
 * - the interrupt service routine code
 *
 * Since we prefer executing the yield() code in a system context, the chosen
 * implementation is with a SCALL, which at least saves 2 registers onto
 * the stack.
 *
 * The INT0..3 code saves a bit more, SR, PC, LR, R08-R12.
 *
 * In order to make multitasking context switching work, we need to save ALL
 * associated data and status registers, in a common way, so that a context
 * saved at yield() can be resumed by an interrupt and the other way around.
 *
 * The chosen task context stack layout is:
 *
 *      R8  (*)         <- TOP, high addresses
 *      R9  (*)
 *      R10 (*)
 *      R11 (*)
 *      R12 (*)
 *      R14/LR (*)
 *      R15/PC (*) (**)
 *      SR (*) (**)
 *      R0
 *      R1
 *      R2
 *      R3
 *      R4
 *      R5
 *      R6
 *      R7              <- BOTTOM, low addresses
 *
 * (*) automatically done for INT0..INT3
 * (**) automatically done for SCALL
 *
 * For this to work, each individual context save/restore code should make
 * the necessary arrangements to create/use the above stack layout.
 *
 * In addition, special care should be given when starting the first task, that
 * requires a distinct contextRestore, matching the stack layout generated by
 * OSSchedulerImpl::stackInitialize().
 *
 * On the other hand, all other tasks are started by regular context switches,
 * so the stack layout generated by OSSchedulerImpl::stackInitialize() should
 * also match the one required by ISR/yield context restore.
 *
 */

#if defined(OS_INCLUDE_OSSCHEDULERIMPL_CONTEXT_PROCESSING)

/*
 *  Save the current SP to the current task, via the global pointer.
 */

#if false

unsigned int getSP(void) __attribute__ ((always_inline));

inline unsigned int getSP(void)
  {
    register unsigned int ret;
    asm volatile
    (
        " ld.w %[RET], sp \n"

        : [RET] "=r" (ret)
        :
        :
    );

    return ret;
  }

inline void
OSSchedulerImpl::stackPointerSave(void)
  {
    *g_ppCurrentStack = (OSStack_t *)getSP();
  }

#else

inline void
OSSchedulerImpl::stackPointerSave(void)
  {
    register unsigned int tmp1; // asm("r8");
    register unsigned int tmp2; // asm("r9");

    asm volatile
    (
        " mov     %[RA], LO(%[pTCB]) \n"
        " orh     %[RA], HI(%[pTCB]) \n" // RA = &g_ppCurrentStack
        " ld.w    %[RB], %[RA][0] \n" // RB = g_ppCurrentStack)
        " st.w    %[RB][0], sp \n" // *g_ppCurrentStack = SP

        : [RA] "=r" (tmp1), [RB] "=r" (tmp2)
        : [pTCB] "i" (&g_ppCurrentStack)
        :
    );
  }

#endif

#if false

unsigned int setSP(void) __attribute__ ((always_inline));

inline void setSP(OSStack_t * p)
  {
    asm volatile
    (
        " st.w %[R], sp \n"
        :
        : [R] "r" (p)
        :
    );
  }

inline void
OSSchedulerImpl::stackPointerRestore(void)
  {
    setSP(*g_ppCurrentStack);
  }

#else

/*
 *  Restore the SP register for the current task, via the global pointer
 */

inline void
OSSchedulerImpl::stackPointerRestore(void)
  {
    register unsigned int tmp1; // asm("r8");
    register unsigned int tmp2; // asm("r9");

    asm volatile
    (
        " mov     %[RA], LO(%[pTCB]) \n"
        " orh     %[RA], HI(%[pTCB]) \n" // RA = &g_ppCurrentStack
        " ld.w    %[RB], %[RA][0] \n" // RB = g_ppCurrentStack)
        " ld.w    sp, %[RB][0] \n" // SP = *g_ppCurrentStack

        : [RA] "=r" (tmp1), [RB] "=r" (tmp2)
        : [pTCB] "i" (&g_ppCurrentStack)
        : "sp"
    );
  }

#endif

/*
 * Save the remaining R0-R7 registers onto the stack.
 */

inline void
OSSchedulerImpl::registersSave(void)
  {
    asm volatile
    (
        " stm     --sp, r0-r7 \n"

        :
        :
        : "sp"
    );
  }

/*
 * Restore the R0-R7 registers from the stack.
 */

inline void
OSSchedulerImpl::registersRestore(void)
  {
    asm volatile
    (
        " ldm     sp++, r0-r7 \n"

        :
        :
        : "sp" // r0-r7 were also changed
    );
  }

/*
 * Check if we are allowed to start a context switch.
 *
 * In general we are not allowed when nested interrupts are used, i.e.
 * a higher priority interrupt occurred in another interrupt context.
 *
 * In this case, except saving/restoring registers, we should NOT attempt
 * anything else, not even storing the SP, since this will overwrite the
 * current task context.
 *
 * In the AVR32 case, the processor mode is available as 3 bits in SR.
 * The value of 0 means user mode, 1 means system mode, both ok.
 * Anything higher than 1 means interrupt or exception mode, when context
 * switch is not allowed.
 *
 */

#if true

unsigned int getSRfromStack(void) __attribute__ ((always_inline));

inline unsigned int getSRfromStack(void)
  {
    register unsigned int ret;
    asm volatile
    (
        " ld.w %[RET], sp[8*4] \n"

        : [RET] "=r" (ret)
        :
        :
    );

    return ret;
  }

inline bool
OSSchedulerImpl::isContextSwitchAllowed(void)
  {
    return ((getSRfromStack() >> 22) & 0x7) > 1 ? false : true;
  }

#else

inline bool
OSSchedulerImpl::isContextSwitchAllowed(void)
  {
    register bool bRet; //asm ("r8");
    register unsigned int tmp;

    // Return 0 (false) if SR mode bits > 1, i.e. in interrupt mode
    // Return 1 (true) if SR mode bits <= 1, i.e. in application or supervisor mode
    asm volatile
    (
        " ld.w    %[RTMP], sp[8*4] \n" // Read SR in stack
        " bfextu  %[RTMP], %[RTMP], 22, 3 \n" // Extract the mode bits to R0.
        " cp.w    %[RTMP], 1 \n" // Compare the mode bits with supervisor mode(b'001)
        " brhi    9f \n" // if interrupt or exception, jump
        " mov     %[RET], 1 \n" // return true
        " bral    8f \n"
        "9: "
        " mov     %[RET], 0 \n" // return false
        "8:"

        : [RTMP] "=r" (tmp), [RET] "=r" (bRet)
        :
        :
    );
    return bRet;
  }
#endif

#endif /* OS_INCLUDE_OSSCHEDULERIMPL_CONTEXT_PROCESSING */

/*
 * Restore the artificially created context for the first task.
 * Used only once at startup; all other tasks will start via a regular
 * context switch.
 *
 * As a side-effect, this function also enables interrupts. This happens when
 * restoring the SR.
 *
 * The processor was (from reset) and remains in system mode.
 *
 * WARNING: Interrupts should be disabled when entering this code.
 *
 */

inline void
OSSchedulerImpl::FirstTask_contextRestore(void)
{
#if false
  register unsigned int tmp1;
  register unsigned int tmp2;

  asm volatile
  (
      " mov     %[RA], LO(%[pTCB]) \n"
      " orh     %[RA], HI(%[pTCB]) \n"
      " ld.w    %[RB], %[RA][0] \n"
      " ld.w    sp, %[RB][0] \n"

      : [RA] "=r" (tmp1), [RB] "=r" (tmp2)
      : [pTCB] "i" (&g_ppCurrentStack)
      : "sp"
  );

  asm volatile
  (
      // Restore R0..R7
      " ldm     sp++, r0-r7 \n"
      // R0-R7 should not be used below this line
      :
      :
      : "sp"
  );

#else

  OSSchedulerImpl::stackPointerRestore();
  OSSchedulerImpl::registersRestore();
  // R1-R7 should not be used below this line

#endif

  asm volatile
  (
      // Skip PC and SR (will do it at the end)
      " sub     sp, -2*4 \n"
      // WARNING: interrupts must be disabled here, otherwise
      // the stack will be clobbered

      // Restore R8..R12 and LR
      " ldm     sp++, r8-r12, lr \n"

      // Restore SR; trash R0, will be restored later
      " ld.w    r0, sp[-8*4] \n"
      " mtsr    %[SR], r0 \n"
      // From this moment on, the interrupt mask is restored to
      // the value set at stackInitialize(), so interrupts might be enabled.

      // Restore R0
      " ld.w    r0, sp[-9*4] \n"

      // Restore PC from stack - PC is the 7th register saved
      " ld.w    pc, sp[-7*4] \n"

      :
      : [SR] "i" (AVR32_SR)
      : "r0", "sp"
  );

}

void
SCALL_contextSave(void) __attribute__( ( always_inline ) );
void
SCALL_contextRestore(void) __attribute__( ( always_inline ) );

/*
 * Push all registers onto the stack and remember position in global variable
 *    *g_ppCurrentStack = SP (stack pointer)
 *
 * Leave interrupts as they were.
 *
 * Warning: the SCALL stack layout doesn't match the one after an interrupt,
 * and in order to make them inter-operate we need to adjust the stack layout.
 *
 * Also, the stack layout depends on the previous mode:
 *
 * If SR[M2:M0] == 001
 *    PC and SR are on the stack.
 * Else (other modes)
 *    Nothing on the stack.
 *
 * However, since it is strictly forbidden to call yield() and eventWait()
 * in an interrupt|exception handler, and all applications run in system
 * mode, we need to consider only the case with PC and SR on stack.
 *
 */

inline void
SCALL_contextSave(void)
{
  asm volatile
  (
      // in order to save R0-R7
      " sub     sp, 6*4 \n"

      // Save R0..R7
      " stm     --sp, r0-r7 \n"

      // in order to save R8-R12 and LR
      // do not use SP if interrupts occurs, SP must be left at bottom of stack
      " sub     r7, sp,-16*4 \n"

      // Copy PC and SR in other places in the stack.
      " ld.w    r0, r7[-2*4] \n" // Read SR
      " st.w    r7[-8*4], r0 \n" // Copy SR

      " ld.w    r0, r7[-1*4] \n" // Read PC
      " st.w    r7[-7*4], r0 \n" // Copy PC

      // Save R8..R12 and LR on the stack.
      " stm     --r7, r8-r12, lr \n"

      // Arriving here we have the following stack layout:
      // R8..R12, LR, PC, SR, R0..R7.
      // Now we can finalise the save.

      :
      :
      :
  );

#if false
  register unsigned int saveSP asm("r10");

  asm volatile
  (
      " mov     %[RSP], sp \n"
      : [RSP] "=r" (saveSP)
      :
      :
  );

  // Enter critical section to protect the global g_ppCurrentStack
  OSScheduler::criticalEnter();

  register unsigned int tmp1 asm("r8");
  register unsigned int tmp2 asm("r9");

  // Store SP in the task area
  asm volatile
  (
      " mov     %[RA], LO(%[pTCB]) \n"
      " orh     %[RA], HI(%[pTCB]) \n"
      " ld.w    %[RB], %[RA][0] \n"
      " st.w    %[RB][0], %[RSP] \n"

      : [RA] "=r" (tmp1),[RB] "=r" (tmp2)
      : [pTCB] "i" (&g_ppCurrentStack), [RSP] "r" (saveSP)
      :
  );
#else

  // enter critical region here; interrupts will be re-enabled
  // when the new context is executed
#if defined(OS_INCLUDE_OSSCHEDULER_CRITICALENTER_WITH_MASK)

  register unsigned int tmp; // asm("r8");
  asm volatile
  (
      " mfsr    %[R], %[SR] \n"
      " orh     %[R], %[MASK] \n" // selectively disable interrupts in MASK
      " mtsr    %[SR], %[R] \n"

      : [R] "=r" (tmp)
      // TODO: define a configuration macro for the MASK
      : [MASK] "i" (0x001E), [SR] "i" (AVR32_SR)
      :
  );

#else
  OSImpl::interruptsDisable();
#endif

  OSSchedulerImpl::stackPointerSave();

  #endif
}

inline void
SCALL_contextRestore(void)
{
#if false
  register unsigned int tmp1 asm("r8");
  register unsigned int tmp2 asm("r9");
  register unsigned int saveSP asm("r10");

  // First restore the stack pointer in a local register saveSP.
  // We cannot use SP now, since criticalEnter/Exit relies on stack.
  asm volatile
  (
      " mov     %[RA], LO(%[pTCB]) \n"
      " orh     %[RA], HI(%[pTCB]) \n"
      " ld.w    %[RB], %[RA][0] \n"
      " ld.w    %[RSP], %[RB][0] \n"

      : [RA] "=r" (tmp1), [RB] "=r" (tmp2), [RSP] "=r" (saveSP)
      : [pTCB] "i" (&g_ppCurrentStack)
      :
  );

  // Exit the critical section used to protect the global g_ppCurrentStack
  OSScheduler::criticalExit();

  asm volatile
    (
        // Restore the saved SP in the stack pointer register
        " mov     sp, %[RSP] \n"

        :
        : [RSP] "r" (saveSP)
        :
    );
#else
  OSSchedulerImpl::stackPointerRestore();
#endif

  asm volatile
  (

      // SP must be left at bottom of the stack, otherwise, in case
      // interrupts occur, the stack content will be clobbered.
      // So for various operations use a local copy in R7
      " sub     r7, sp, -10*4 \n"
      // R7 now points to LR location

      // Restore r8-r12 and LR
      " ldm     r7++, r8-r12, lr \n"
      // R7 now points to the top

      // RETS will take care of the extra PC and SR restore.
      // So, we have to prepare the stack as required for RETS.
      " ld.w    r0, r7[-8*4] \n" // Read SR
      " st.w    r7[-2*4], r0 \n" // Copy SR
      " ld.w    r0, r7[-7*4] \n" // Read PC
      " st.w    r7[-1*4], r0 \n" // Copy PC

      // Restore R0..R7
      " ldm     sp++, r0-r7 \n"

      // Adjust SP to point to new location of SR & PC
      " sub     sp, -6*4 \n"

      // Finally return from SCALL
      " rets \n"

      :
      :
      :
  );
}

/*
 * Critical section management.
 *
 * Since critical sections can be nested, we store the saved values onto
 * the stack.
 *
 * Notice: The function context is addressed via R07, not SP, so using the
 * stack as temporary storage should be safe.
 *
 * There were some rumours about the compiler aggressively optimising the stack,
 * leading to strange behaviours, but we cannot confirm them.
 */

/*
 * Push SR onto the stack and disable interrupts.
 *
 * To allow Real-Time interrupts to run, we do not disable all interrupts,
 * but selectively mask only the non-Real-Time levels.
 *
 */

inline void
OSScheduler::criticalEnter(void)
{
#if !defined(OS_EXCLUDE_MULTITASKING)
  register unsigned int tmp; //asm("r8");

  asm volatile
  (
      " mfsr    %[R], %[SR] \n"
      " st.w    --sp, %[R] \n" // push value onto stack

#if defined(OS_INCLUDE_OSSCHEDULER_CRITICALENTER_WITH_MASK)
      " orh     %[R], %[MASK] \n" // selectively disable interrupts in MASK
      " mtsr    %[SR], %[R] \n"
#else
      " ssrf    %[GM] \n" // disable all interrupts
#endif

      : [R] "=r" (tmp)
      // TODO: define a configuration macro for the MASK
      : [MASK] "i" (0x001E), [SR] "i" (AVR32_SR), [GM] "i" (AVR32_SR_GM_OFFSET)
      :
  );
#endif
}

/*
 * Pop SR from the stack; this will also restore interrupts to
 * their previous state.
 *
 */

inline void
OSScheduler::criticalExit(void)
{
#if !defined(OS_EXCLUDE_MULTITASKING)
  register unsigned int tmp; // asm("r8");

  asm volatile
  (
      " ld.w    %[R], sp++ \n"
      " mtsr    %[SR], %[R] \n"

      : [R] "=r" (tmp)
      : [SR] "i" (AVR32_SR)
      : "cc" // "cc" mean configuration flags
  );
#endif
}

/*
 * Perform a context switch by triggering a system call exception.
 *
 */

inline void
OSSchedulerImpl::yield(void)
{
#if defined(OS_INCLUDE_OSSCHEDULER_YIELD_UNDER_CONSTRUCTION)
  ; // test code here
#else
  asm volatile
  (
      " scall \n"
      :
      :
      :
  );
#endif
}

#endif /*HAL_FAMILY_OSSCHEDULER_INLINES_H_*/
