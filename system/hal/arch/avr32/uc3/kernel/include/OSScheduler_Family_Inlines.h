/*
 *      Copyright (C) 2011 Liviu Ionescu.
 *
 *	This file is part of the uOS++ distribution.
 *
 *	Many thanks to the FreeRTOS developers for providing great
 *	inspiration and functional pieces of code.
 */

#ifndef HAL_FAMILY_OSSCHEDULER_INLINES_H_
#define HAL_FAMILY_OSSCHEDULER_INLINES_H_

// ----------------------------------------------------------------------------

/*
 * Overview
 *
 * These definitions handle the operations required to save and restore the
 * current thread 'context' to/from the thread control block (TCB).
 *
 * The processor registers are pushed onto/popped from the current thread stack,
 * and only the resulting stack pointer (SP) is stored in the current thread area.
 *
 * To make things simpler and faster, the code uses the global variable
 * OSScheduler::ms_ppCurrentStack that points to the current OSThread::m_pStack, where
 * the pointer to the current stack frame is located. This variable is
 * set during the context switch in OSScheduler::contextSwitch().
 *
 * - contextSave() : store the SP through the global pointer
 *      *OSScheduler::ms_ppCurrentStack = SP
 *
 * - contextRestore() : load the SP through the global pointer
 *      SP = *OSScheduler::ms_ppCurrentStack
 */

/*
 * AVR32 Specifics
 *
 * On AVR32 there is no special, unified, support for context switching,
 * (like on ARM Cortex M3/M4), do we have two distinct context switching
 * situations:
 *
 * - the yield() code
 * - the interrupt service routine code
 *
 * Since we prefer executing the yield() code in a system context, the chosen
 * implementation is with a SCALL, which at least saves 2 registers onto
 * the stack.
 *
 * The INT0..3 code saves a bit more, SR, PC, LR, R08-R12.
 *
 * In order to make multithreading context switching work, we need to save ALL
 * associated data and status registers, in a common way, so that a context
 * saved at yield() can be resumed by an interrupt and the other way around.
 *
 * The chosen thread context stack layout is:
 *
 *      R8  (*)         <- TOP, high addresses
 *      R9  (*)
 *      R10 (*)
 *      R11 (*)
 *      R12 (*)
 *      R14/LR (*)
 *      R15/PC (*) (**)
 *      SR (*) (**)
 *      R0
 *      R1
 *      R2
 *      R3
 *      R4
 *      R5
 *      R6
 *      R7
 *      criticalSectionNesting   <- BOTTOM, low addresses
 *
 * (*) automatically done for INT0..INT3
 * (**) automatically done for SCALL
 *
 * For this to work, each individual context save/restore code should make
 * the necessary arrangements to create/use the above stack layout.
 *
 * In addition, special care should be given when starting the first thread, that
 * requires a distinct contextRestore, matching the stack layout generated by
 * OSSchedulerImpl::stackInitialise().
 *
 * On the other hand, all other threads are started by regular context switches,
 * so the stack layout generated by OSSchedulerImpl::stackInitialise() should
 * also match the one required by ISR/yield context restore.
 *
 */

#if defined(OS_INCLUDE_OSSCHEDULERIMPL_CONTEXT_PROCESSING)

/*
 *  Save the current SP to the current thread, via the global pointer.
 */

#if false

unsigned int getSP(void) __attribute__ ((always_inline));

inline unsigned int getSP(void)
  {
    register unsigned int ret;
    asm volatile
    (
        " ld.w %[RET], sp \n"

        : [RET] "=r" (ret)
        :
        :
    );

    return ret;
  }

inline void
OSSchedulerImpl::stackPointerSave(void)
  {
    *OSScheduler::ms_ppCurrentStack = (OSStack_t *)getSP();
  }

#else

inline void
OSSchedulerImpl::stackPointerSave(void)
  {
#if !defined(OS_EXCLUDE_MULTITASKING)

    register unsigned int tmp1;
    register unsigned int tmp2;

    asm volatile
    (
        " mov     %[RA], LO(%[pTCB]) \n"
        " orh     %[RA], HI(%[pTCB]) \n" // RA = &OSScheduler::ms_ppCurrentStack
        " ld.w    %[RB], %[RA][0] \n" // RB = OSScheduler::ms_ppCurrentStack)
        " st.w    %[RB][0], sp \n" // *OSScheduler::ms_ppCurrentStack = SP

        : [RA] "=r" (tmp1), [RB] "=r" (tmp2)
        : [pTCB] "i" (&OSScheduler::ms_ppCurrentStack)
        :
    );

#endif /* !defined(OS_EXCLUDE_MULTITASKING) */
  }

#endif

#if false

unsigned int setSP(void) __attribute__ ((always_inline));

inline void setSP(OSStack_t * p)
  {
    asm volatile
    (
        " st.w %[R], sp \n"
        :
        : [R] "r" (p)
        :
    );
  }

inline void
OSSchedulerImpl::stackPointerRestore(void)
  {
    setSP(*OSScheduler::ms_ppCurrentStack);
  }

#else

/*
 *  Restore the SP register for the current thread, via the global pointer
 */

inline void
OSSchedulerImpl::stackPointerRestore(void)
  {
#if !defined(OS_EXCLUDE_MULTITASKING)

    register unsigned int tmp1;
    register unsigned int tmp2;

    asm volatile
    (
        " mov     %[RA], LO(%[pTCB]) \n"
        " orh     %[RA], HI(%[pTCB]) \n" // RA = &OSScheduler::ms_ppCurrentStack
        " ld.w    %[RB], %[RA][0] \n" // RB = OSScheduler::ms_ppCurrentStack)
        " ld.w    sp, %[RB][0] \n" // SP = *OSScheduler::ms_ppCurrentStack

        : [RA] "=r" (tmp1), [RB] "=r" (tmp2)
        : [pTCB] "i" (&OSScheduler::ms_ppCurrentStack)
        : "sp"
    );

#endif /* !defined(OS_EXCLUDE_MULTITASKING) */
  }

#endif

/*
 * Save the global critical section nesting counter onto stack
 */

inline void
OSSchedulerImpl::criticalSectionNestingSave(void)
  {
#if !defined(OS_EXCLUDE_MULTITASKING)

    register unsigned int tmp1;
    register unsigned int tmp2;

    asm volatile
    (
        " mov     %[RA], LO(%[pCSN]) \n"
        " orh     %[RA], HI(%[pCSN]) \n"
        " ld.w    %[RB], %[RA][0] \n"
        " st.w    --sp, %[RB] \n " // push Nesting onto stack

        : [RA] "=r" (tmp1), [RB] "=r" (tmp2)
        : [pCSN] "i" (&OSCriticalSection::ms_nestingLevel)
        :
    );

#endif /* !defined(OS_EXCLUDE_MULTITASKING) */
  }

/*
 *  Restore the global critical section nesting counter from stack
 */

inline void
OSSchedulerImpl::criticalSectionNestingRestore(void)
  {
#if !defined(OS_EXCLUDE_MULTITASKING)

    register unsigned int tmp1;
    register unsigned int tmp2;

    asm volatile
    (
        " ld.w    %[RA], sp++ \n" // pop Nesting from stack
        " mov     %[RB], LO(%[pCSN]) \n"
        " orh     %[RB], HI(%[pCSN]) \n"
        " st.w    %[RB][0], %[RA] \n"

        : [RA] "=r" (tmp1), [RB] "=r" (tmp2)
        : [pCSN] "i" (&OSCriticalSection::ms_nestingLevel)
        : "sp"
    );

#endif /* !defined(OS_EXCLUDE_MULTITASKING) */
  }

/*
 * Save the remaining R0-R7 registers onto the stack.
 */

inline void
OSSchedulerImpl::registersSave(void)
  {
    asm volatile
    (
        " stm     --sp, r0-r7 \n"

        :
        :
        : "sp"
    );
  }

/*
 * Restore the R0-R7 registers from the stack.
 */

inline void
OSSchedulerImpl::registersRestore(void)
  {
    asm volatile
    (
        " ldm     sp++, r0-r7 \n"

        :
        :
        : "sp" // r0-r7 were also changed
    );
  }

/*
 * Check if we are allowed to start a context switch.
 *
 * In general we are not allowed when nested interrupts are used, i.e.
 * a higher priority interrupt occurred in another interrupt context.
 *
 * In this case, except saving/restoring registers, we should NOT attempt
 * anything else, not even storing the SP, since this will overwrite the
 * current thread context.
 *
 * In the AVR32 case, the processor mode is available as 3 bits in SR.
 * The value of 0 means user mode, 1 means system mode, both ok.
 * Anything higher than 1 means interrupt or exception mode, when context
 * switch is not allowed.
 *
 */

unsigned int getSRfromStack(void) __attribute__ ((always_inline));

inline unsigned int getSRfromStack(void)
  {
    register unsigned int ret;
    asm volatile
    (
        " ld.w %[RET], sp[9*4] \n" // 8 without nesting

        : [RET] "=r" (ret)
        :
        :
    );

    return ret;
  }

inline bool
OSSchedulerImpl::isContextSwitchAllowed(void)
  {
    return ((getSRfromStack() >> 22) & 0x7) > 1 ? false : true;
  }

#endif /* OS_INCLUDE_OSSCHEDULERIMPL_CONTEXT_PROCESSING */

/*
 * Restore the artificially created context for the first thread.
 * Used only once at startup; all other threads will start via a regular
 * context switch.
 *
 * As a side-effect, this function also enables interrupts. This happens when
 * restoring the SR.
 *
 * The processor was (from reset) and remains in system mode.
 *
 * WARNING: Interrupts should be disabled when entering this code.
 *
 */

inline void
OSSchedulerImpl::FirstThread_contextRestore(void)
{

  OSSchedulerImpl::stackPointerRestore();
  OSSchedulerImpl::criticalSectionNestingRestore();
  OSSchedulerImpl::registersRestore();
  // R1-R7 should not be used below this line

  asm volatile
  (
      // Skip PC and SR (will do it at the end)
      " sub     sp, -2*4 \n"
      // WARNING: interrupts must be disabled here, otherwise
      // the stack will be clobbered

      // Restore R8..R12 and LR
      " ldm     sp++, r8-r12, lr \n"

      // Restore SR; trash R0, will be restored later
      " ld.w    r0, sp[-8*4] \n"
      " mtsr    %[SR], r0 \n"
      // From this moment on, the interrupt mask is restored to
      // the value set at stackInitialise(), so interrupts might be enabled.

      // Restore R0
      " ld.w    r0, sp[-9*4] \n"

      // Restore PC from stack - PC is the 7th register saved
      " ld.w    pc, sp[-7*4] \n"

      :
      : [SR] "i" (AVR32_SR)
      : "r0", "sp"
  );

}

void
SCALL_contextSave(void) __attribute__((always_inline));
void
SCALL_contextRestore(void) __attribute__((always_inline));

/*
 * Push all registers onto the stack and remember position in global variable
 *    *OSScheduler::ms_ppCurrentStack = SP (stack pointer)
 *
 * Leave interrupts as they were.
 *
 * Warning: the SCALL stack layout doesn't match the one after an interrupt,
 * and in order to make them inter-operate we need to adjust the stack layout.
 *
 * Also, the stack layout depends on the previous mode:
 *
 * If SR[M2:M0] == 001
 *    PC and SR are on the stack.
 * Else (other modes)
 *    Nothing on the stack.
 *
 * However, since it is strictly forbidden to call yield() and eventWait()
 * in an interrupt|exception handler, and all applications run in system
 * mode, we need to consider only the case with PC and SR on stack.
 *
 */

inline void
SCALL_contextSave(void)
{
  asm volatile
  (
      // in order to save R0-R7
      " sub     sp, 6*4 \n"

      // Save R0..R7
      " stm     --sp, r0-r7 \n"

      // in order to save R8-R12 and LR
      // do not use SP if interrupts occurs, SP must be left at bottom of stack
      " sub     r7, sp,-16*4 \n"

      // Copy PC and SR in other places in the stack.
      " ld.w    r0, r7[-2*4] \n" // Read SR
      " st.w    r7[-8*4], r0 \n" // Copy SR

      " ld.w    r0, r7[-1*4] \n" // Read PC
      " st.w    r7[-7*4], r0 \n" // Copy PC

      // Save R8..R12 and LR on the stack.
      " stm     --r7, r8-r12, lr \n"

      // Arriving here we have the following stack layout:
      // R8..R12, LR, PC, SR, R0..R7.
      // Now we can finalise the save.

      :
      :
      :
  );

  OSSchedulerImpl::criticalSectionNestingSave();

  // enter critical region here; interrupts will be re-enabled
  // when the new context is executed

#if defined(OS_INCLUDE_OSSCHEDULER_CRITICALENTER_WITH_MASK)

#if false
  register unsigned int tmp; // asm("r8");
  asm volatile
  (
      " mfsr    %[R], %[SR] \n"
      " orh     %[R], %[MASK] \n" // selectively disable interrupts in MASK
      " mtsr    %[SR], %[R] \n"

      : [R] "=r" (tmp)
      // TODO: define a configuration macro for the MASK
      : [MASK] "i" (0x0F << 1), [SR] "i" (AVR32_SR)
      :
  );
#else
  register OSStack_t tmp;

  tmp = OSCPUImpl::getInterruptsMask();
  tmp |= (OS_CFGINT_OSCRITICALSECTION_MASK);
  OSCPUImpl::setInterruptsMask(tmp);
#endif

#else
  OSCPUImpl::interruptsDisable();
#endif

  OSSchedulerImpl::stackPointerSave();
}

inline void
SCALL_contextRestore(void)
{
  OSSchedulerImpl::stackPointerRestore();
  OSSchedulerImpl::criticalSectionNestingRestore();

  asm volatile
  (

      // SP must be left at bottom of the stack, otherwise, in case
      // interrupts occur, the stack content will be clobbered.
      // So for various operations use a local copy in R7
      " sub     r7, sp, -10*4 \n"
      // R7 now points to LR location

      // Restore r8-r12 and LR
      " ldm     r7++, r8-r12, lr \n"
      // R7 now points to the top

      // RETS will take care of the extra PC and SR restore.
      // So, we have to prepare the stack as required for RETS.

      // TODO: check if needed
      // Here we might need to exit the critical section. (FreeRTOS does it)

      " ld.w    r0, r7[-8*4] \n" // Read SR
      " st.w    r7[-2*4], r0 \n" // Copy SR
      " ld.w    r0, r7[-7*4] \n" // Read PC
      " st.w    r7[-1*4], r0 \n" // Copy PC

      // Restore R0..R7
      " ldm     sp++, r0-r7 \n"

      // Adjust SP to point to new location of SR & PC
      " sub     sp, -6*4 \n"

      // Finally return from SCALL
      " rets \n"

      :
      :
      :
  );
}

/*
 * Perform a context switch by triggering a system call exception.
 */

inline void
OSSchedulerImpl::yield(void)
{
  asm volatile
  (
      " scall \n"
      :
      :
      :
  );
}

#endif /* HAL_FAMILY_OSSCHEDULER_INLINES_H_ */
